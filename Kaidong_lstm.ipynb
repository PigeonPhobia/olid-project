{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from utils import preprocess, plotutils\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from spacy.lang.en import stop_words\n",
    "from string import punctuation\n",
    "import emoji\n",
    "from utils import preprocess\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86426</td>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90194</td>\n",
       "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16820</td>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62688</td>\n",
       "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43605</td>\n",
       "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet subtask_a  \\\n",
       "0  86426  @USER She should ask a few native Americans wh...       OFF   \n",
       "1  90194  @USER @USER Go home you’re drunk!!! @USER #MAG...       OFF   \n",
       "2  16820  Amazon is investigating Chinese employees who ...       NOT   \n",
       "3  62688  @USER Someone should'veTaken\" this piece of sh...       OFF   \n",
       "4  43605  @USER @USER Obama wanted liberals &amp; illega...       NOT   \n",
       "\n",
       "  subtask_b subtask_c  \n",
       "0       UNT       NaN  \n",
       "1       TIN       IND  \n",
       "2       NaN       NaN  \n",
       "3       UNT       NaN  \n",
       "4       NaN       NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = \"./data/\"\n",
    "TRAIN_DATA_FILE = DATA_DIR + \"olid-training-v1.0.tsv\"\n",
    "ori_train_data = pd.read_csv(TRAIN_DATA_FILE, sep='\\t')\n",
    "ori_train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_label(a, b, c):\n",
    "    if a == 'NOT':\n",
    "        return a\n",
    "    elif b == 'UNT':\n",
    "        return b\n",
    "    else:\n",
    "        return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_train_data['label'] = ori_train_data.apply(lambda x: compute_label(x.subtask_a, x.subtask_b, x.subtask_c), axis=1)\n",
    "count = ori_train_data.groupby('label').size().to_dict()\n",
    "stop_words = stop_words.STOP_WORDS\n",
    "stop_words = stop_words.union(set(emoji.get_emoji_unicode_dict('en').values()))\n",
    "stop_words.add(\"url\")\n",
    "stop_words = stop_words.union(set(punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_tokenization(doc):\n",
    "    tweet_tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, match_phone_numbers=False)\n",
    "    tweet_tokenize = [tweet_tokenizer.tokenize(tweet) for tweet in doc]\n",
    "    return tweet_tokenize\n",
    "\n",
    "def stop_word_removal(doc, stop_words):\n",
    "    new_tweets = []\n",
    "    for t in doc:\n",
    "        new_words = []\n",
    "        for word in t:\n",
    "            if word not in stop_words:\n",
    "                new_words.append(word)\n",
    "        new_tweets.append(new_words)\n",
    "\n",
    "    return new_tweets\n",
    "\n",
    "def hashtag_removal(doc):\n",
    "    new_docs = []\n",
    "    for t in doc:\n",
    "        new_doc=[]\n",
    "        for word in t:\n",
    "            if word[0] == \"#\":\n",
    "                new_doc.append(word[1:])\n",
    "            else:\n",
    "                new_doc.append(word)\n",
    "        new_docs.append(new_doc)\n",
    "    \n",
    "    return new_docs\n",
    "\n",
    "tweets = ori_train_data[\"tweet\"]\n",
    "tweet_tokenization = tweet_tokenization(tweets)\n",
    "tweet_remove_stop_word = stop_word_removal(tweet_tokenization, stop_words)\n",
    "tweets_remove = hashtag_removal(tweet_remove_stop_word)\n",
    "tweets_remove = [\" \".join(t) for t in tweets_remove]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: 10592\n",
      "Size of test set: 2648\n"
     ]
    }
   ],
   "source": [
    "label2id = {'NOT': 0, 'OFF': 1}\n",
    "sub_a_label = ori_train_data['subtask_a'].map(label2id).to_list()\n",
    "sentences_train, sentences_test, labels_train, labels_test = train_test_split(tweets_remove, sub_a_label, test_size=0.2, random_state=5246)\n",
    "\n",
    "y_train = np.asarray(labels_train)\n",
    "y_test = np.asarray(labels_test)\n",
    "\n",
    "print(\"Size of training set: {}\".format(len(sentences_train)))\n",
    "print(\"Size of test set: {}\".format(len(sentences_test)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/331], Loss: 0.7050\n",
      "Epoch [1/10], Step [200/331], Loss: 0.5693\n",
      "Epoch [1/10], Step [300/331], Loss: 0.4136\n",
      "Epoch [2/10], Step [100/331], Loss: 0.2369\n",
      "Epoch [2/10], Step [200/331], Loss: 0.1267\n",
      "Epoch [2/10], Step [300/331], Loss: 0.2413\n",
      "Epoch [3/10], Step [100/331], Loss: 0.1408\n",
      "Epoch [3/10], Step [200/331], Loss: 0.0646\n",
      "Epoch [3/10], Step [300/331], Loss: 0.0280\n",
      "Epoch [4/10], Step [100/331], Loss: 0.0337\n",
      "Epoch [4/10], Step [200/331], Loss: 0.0106\n",
      "Epoch [4/10], Step [300/331], Loss: 0.0150\n",
      "Epoch [5/10], Step [100/331], Loss: 0.0999\n",
      "Epoch [5/10], Step [200/331], Loss: 0.0177\n",
      "Epoch [5/10], Step [300/331], Loss: 0.0711\n",
      "Epoch [6/10], Step [100/331], Loss: 0.0022\n",
      "Epoch [6/10], Step [200/331], Loss: 0.0011\n",
      "Epoch [6/10], Step [300/331], Loss: 0.0768\n",
      "Epoch [7/10], Step [100/331], Loss: 0.0086\n",
      "Epoch [7/10], Step [200/331], Loss: 0.0053\n",
      "Epoch [7/10], Step [300/331], Loss: 0.0156\n",
      "Epoch [8/10], Step [100/331], Loss: 0.0016\n",
      "Epoch [8/10], Step [200/331], Loss: 0.0011\n",
      "Epoch [8/10], Step [300/331], Loss: 0.0305\n",
      "Epoch [9/10], Step [100/331], Loss: 0.0044\n",
      "Epoch [9/10], Step [200/331], Loss: 0.0004\n",
      "Epoch [9/10], Step [300/331], Loss: 0.0112\n",
      "Epoch [10/10], Step [100/331], Loss: 0.0179\n",
      "Epoch [10/10], Step [200/331], Loss: 0.0007\n",
      "Epoch [10/10], Step [300/331], Loss: 0.0032\n",
      "Test Accuracy: 72.32%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         NOT       0.76      0.84      0.80      1750\n",
      "         OFF       0.61      0.50      0.55       898\n",
      "\n",
      "    accuracy                           0.72      2648\n",
      "   macro avg       0.69      0.67      0.67      2648\n",
      "weighted avg       0.71      0.72      0.72      2648\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, vectorizer):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.vectorizer = vectorizer\n",
    "        self.sequences = self.vectorizer.transform(sentences).toarray()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sequences[idx]), torch.tensor(self.labels[idx])\n",
    "\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(1, batch_size, self.hidden_dim).requires_grad_()\n",
    "        c0 = torch.zeros(1, batch_size, self.hidden_dim).requires_grad_()\n",
    "\n",
    "        x = x.view(batch_size, 1, -1)\n",
    "\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        out = self.fc(hn[-1])\n",
    "        return out\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english', lowercase=True)\n",
    "X_train = vectorizer.fit_transform(sentences_train)\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dim = 100\n",
    "output_dim = 2\n",
    "lr = 0.01\n",
    "num_epochs = 10\n",
    "\n",
    "\n",
    "model = LSTMClassifier(input_dim, hidden_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "train_dataset = TextDataset(sentences_train, y_train, vectorizer)\n",
    "test_dataset = TextDataset(sentences_test, y_test, vectorizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    hn, cn = None, None\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        if hn is not None and cn is not None:\n",
    "            hn, cn = hn.detach(), cn.detach()\n",
    "        outputs = model(inputs.float())\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        hn, cn = outputs[1][0].detach(), outputs[1][1].detach()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "hn, cn = None, None\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    pred = []\n",
    "    for i, (inputs, labels) in enumerate(test_loader):\n",
    "\n",
    "        outputs = model(inputs.float())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        pred.append(predicted)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "y_pred = np.asarray(torch.cat(pred))\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=['NOT', 'OFF']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pack Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [100/331], Loss: 0.5573\n",
      "Epoch [1/3], Step [200/331], Loss: 0.5935\n",
      "Epoch [1/3], Step [300/331], Loss: 0.4664\n",
      "Epoch [2/3], Step [100/331], Loss: 0.3889\n",
      "Epoch [2/3], Step [200/331], Loss: 0.4775\n",
      "Epoch [2/3], Step [300/331], Loss: 0.2932\n",
      "Epoch [3/3], Step [100/331], Loss: 0.2257\n",
      "Epoch [3/3], Step [200/331], Loss: 0.2169\n",
      "Epoch [3/3], Step [300/331], Loss: 0.1594\n",
      "Test Accuracy: 73.75%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         NOT       0.78      0.85      0.81      1750\n",
      "         OFF       0.64      0.52      0.57       898\n",
      "\n",
      "    accuracy                           0.74      2648\n",
      "   macro avg       0.71      0.69      0.69      2648\n",
      "weighted avg       0.73      0.74      0.73      2648\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(batch):\n",
    "    batch.sort(key=lambda x: x[2], reverse=True)\n",
    "    sequences, labels, lengths = zip(*batch)\n",
    "    sequences = nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=0.0)\n",
    "    return sequences, torch.tensor(labels), torch.tensor(lengths)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, vectorizer, input_dim):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.vectorizer = vectorizer\n",
    "        self.sequences = [torch.tensor(vectorizer.transform([sentence]).toarray()).reshape(-1, input_dim) for sentence in sentences]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], torch.tensor(self.labels[idx]), len(self.sequences[idx])\n",
    "\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(1, batch_size, self.hidden_dim).requires_grad_()\n",
    "        c0 = torch.zeros(1, batch_size, self.hidden_dim).requires_grad_()\n",
    "\n",
    "        x = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True)\n",
    "\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        out = self.fc(hn[-1])\n",
    "        return out\n",
    "\n",
    "    \n",
    "vectorizer = CountVectorizer(stop_words='english', lowercase=True)\n",
    "X_train = vectorizer.fit_transform(sentences_train)\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dim = 100\n",
    "output_dim = 2\n",
    "lr = 0.001\n",
    "num_epochs = 3\n",
    "\n",
    "\n",
    "model = LSTMClassifier(input_dim, hidden_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "train_dataset = TextDataset(sentences_train, y_train, vectorizer, input_dim)\n",
    "test_dataset = TextDataset(sentences_test, y_test, vectorizer, input_dim)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels, lengths) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs.float(), lengths)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    pred = []\n",
    "    for i, (inputs, labels, lengths) in enumerate(test_loader):\n",
    "        outputs = model(inputs.float(), lengths)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        pred.append(predicted)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "y_pred = np.asarray(torch.cat(pred))\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=['NOT', 'OFF']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data/\"\n",
    "TRAIN_DATA_FILE = DATA_DIR + \"olid-training-v1.0.tsv\"\n",
    "TEST_A_DATA_FILE = DATA_DIR + \"testset-levela.tsv\"\n",
    "LABEL_A_DATA_FILE = DATA_DIR + \"labels-levela.csv\"\n",
    "TEST_B_DATA_FILE = DATA_DIR + \"testset-levelb.tsv\"\n",
    "LABEL_B_DATA_FILE = DATA_DIR + \"labels-levelb.csv\"\n",
    "TEST_C_DATA_FILE = DATA_DIR + \"testset-levelc.tsv\"\n",
    "LABEL_C_DATA_FILE = DATA_DIR + \"labels-levelc.csv\"\n",
    "\n",
    "ori_train_data = pd.read_csv(TRAIN_DATA_FILE, sep='\\t')\n",
    "\n",
    "task_a_test_data = pd.read_csv(TEST_A_DATA_FILE, sep='\\t')\n",
    "task_a_test_label = pd.read_csv(LABEL_A_DATA_FILE, usecols=[1], names=['labela'])\n",
    "\n",
    "task_b_test_data = pd.read_csv(TEST_B_DATA_FILE, sep='\\t')\n",
    "task_b_test_label = pd.read_csv(LABEL_B_DATA_FILE, usecols=[1], names=['labelb'])\n",
    "\n",
    "task_c_test_data = pd.read_csv(TEST_C_DATA_FILE, sep='\\t')\n",
    "task_c_test_label = pd.read_csv(LABEL_C_DATA_FILE, usecols=[1], names=['labelc'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([19034, 25])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_list = ori_train_data['tweet'].to_list()\n",
    "processed_tweet_list = [preprocess.glove_twitter_preprocess(t) for t in tweet_list]\n",
    "tokenized_tweet_list = preprocess.nltk_tokenize(processed_tweet_list)\n",
    "vocabulary, documents_vector = preprocess.transform_word_to_vector(tokenized_tweet_list, num_vocab=20000, padded=False)\n",
    "\n",
    "pretrained_embedding = preprocess.get_embedding_from_torch_text(vocabulary, \"twitter.27B\", 25)\n",
    "pretrained_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {'NOT': 0, 'OFF': 1}\n",
    "sub_a_label = ori_train_data['subtask_a'].map(label2id).to_list()\n",
    "sentences_train, sentences_test, labels_train, labels_test = train_test_split(documents_vector, sub_a_label, test_size=0.2, random_state=5246)\n",
    "y_train = np.asarray(labels_train)\n",
    "y_test = np.asarray(labels_test)\n",
    "\n",
    "count_not, count_off= np.sum(y_train == 0), np.sum(y_train == 1)\n",
    "weight_not = len(y_train) / (2 * count_not)\n",
    "weight_off = len(y_train) / (2 * count_off)\n",
    "class_weights = torch.FloatTensor([weight_not, weight_off])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    batch.sort(key=lambda x: x[2], reverse=True)\n",
    "    sequences, labels, lengths = zip(*batch)\n",
    "    sequences = nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=0.0)\n",
    "    return sequences, torch.tensor(labels), torch.tensor(lengths)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, sentences, labels):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx], torch.tensor(self.labels[idx]), len(self.sentences[idx])\n",
    "\n",
    "# class LSTMClassifier(nn.Module):\n",
    "#     def __init__(self, embedding_dim, hidden_dim, output_dim, pretrained_embedding):\n",
    "#         super(LSTMClassifier, self).__init__()\n",
    "\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.embedding = nn.Embedding.from_pretrained(pretrained_embedding)\n",
    "#         self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "#         self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "#     def forward(self, x, lengths):\n",
    "#         batch_size = x.size(0)\n",
    "#         x = self.embedding(x)\n",
    "#         h0 = torch.zeros(1, batch_size, self.hidden_dim).requires_grad_()\n",
    "#         c0 = torch.zeros(1, batch_size, self.hidden_dim).requires_grad_()\n",
    "\n",
    "#         x = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True)\n",
    "\n",
    "#         out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "#         out, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "#         out = self.fc(hn[-1])\n",
    "#         return out\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim, pretrained_embedding):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embedding)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(2 * hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        batch_size = x.size(0)\n",
    "        x = self.embedding(x)\n",
    "        h0 = torch.zeros(2, batch_size, self.hidden_dim).requires_grad_()\n",
    "        c0 = torch.zeros(2, batch_size, self.hidden_dim).requires_grad_()\n",
    "\n",
    "        x = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True)\n",
    "\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        out = self.fc(torch.cat((hn[-2], hn[-1]), dim=1))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/331], Loss: 0.5837\n",
      "Epoch [1/10], Step [200/331], Loss: 0.6062\n",
      "Epoch [1/10], Step [300/331], Loss: 0.6370\n",
      "Epoch [2/10], Step [100/331], Loss: 0.5525\n",
      "Epoch [2/10], Step [200/331], Loss: 0.5903\n",
      "Epoch [2/10], Step [300/331], Loss: 0.4903\n",
      "Epoch [3/10], Step [100/331], Loss: 0.5850\n",
      "Epoch [3/10], Step [200/331], Loss: 0.4008\n",
      "Epoch [3/10], Step [300/331], Loss: 0.4418\n",
      "Epoch [4/10], Step [100/331], Loss: 0.5595\n",
      "Epoch [4/10], Step [200/331], Loss: 0.4050\n",
      "Epoch [4/10], Step [300/331], Loss: 0.3465\n",
      "Epoch [5/10], Step [100/331], Loss: 0.4745\n",
      "Epoch [5/10], Step [200/331], Loss: 0.4319\n",
      "Epoch [5/10], Step [300/331], Loss: 0.4768\n",
      "Epoch [6/10], Step [100/331], Loss: 0.4479\n",
      "Epoch [6/10], Step [200/331], Loss: 0.7108\n",
      "Epoch [6/10], Step [300/331], Loss: 0.5251\n",
      "Epoch [7/10], Step [100/331], Loss: 0.3425\n",
      "Epoch [7/10], Step [200/331], Loss: 0.3834\n",
      "Epoch [7/10], Step [300/331], Loss: 0.4343\n",
      "Epoch [8/10], Step [100/331], Loss: 0.3453\n",
      "Epoch [8/10], Step [200/331], Loss: 0.3376\n",
      "Epoch [8/10], Step [300/331], Loss: 0.5356\n",
      "Epoch [9/10], Step [100/331], Loss: 0.3917\n",
      "Epoch [9/10], Step [200/331], Loss: 0.4924\n",
      "Epoch [9/10], Step [300/331], Loss: 0.3572\n",
      "Epoch [10/10], Step [100/331], Loss: 0.4617\n",
      "Epoch [10/10], Step [200/331], Loss: 0.4632\n",
      "Epoch [10/10], Step [300/331], Loss: 0.4048\n",
      "Test Accuracy: 76.47%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         NOT       0.82      0.83      0.82      1750\n",
      "         OFF       0.66      0.65      0.65       898\n",
      "\n",
      "    accuracy                           0.76      2648\n",
      "   macro avg       0.74      0.74      0.74      2648\n",
      "weighted avg       0.76      0.76      0.76      2648\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = pretrained_embedding.size(1)\n",
    "hidden_dim = 64\n",
    "output_dim = 2\n",
    "lr = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "model = LSTMClassifier(embedding_dim, hidden_dim, output_dim, pretrained_embedding)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-3)\n",
    "\n",
    "train_dataset = TextDataset(sentences_train, y_train)\n",
    "test_dataset = TextDataset(sentences_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels, lengths) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs.long(), lengths)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    pred, label = [],[]\n",
    "    for i, (inputs, labels, lengths) in enumerate(test_loader):\n",
    "        outputs = model(inputs.long(), lengths)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        pred.append(predicted)\n",
    "        label.append(labels)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "y_pred = np.asarray(torch.cat(pred))\n",
    "y_label = np.asarray(torch.cat(label))\n",
    "print(metrics.classification_report(y_label, y_pred, target_names=['NOT', 'OFF']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test task a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 77.91%\n",
      "0.7790697674418605\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         NOT       0.81      0.91      0.86       620\n",
      "         OFF       0.65      0.45      0.53       240\n",
      "\n",
      "    accuracy                           0.78       860\n",
      "   macro avg       0.73      0.68      0.69       860\n",
      "weighted avg       0.77      0.78      0.76       860\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_test_a = task_a_test_data[\"tweet\"].tolist()\n",
    "processed_tweet_list_a = [preprocess.glove_twitter_preprocess(t) for t in tweets_test_a]\n",
    "tokenized_tweet_list_a = preprocess.nltk_tokenize(processed_tweet_list_a)\n",
    "# _ , documents_vector_a = preprocess.transform_word_to_vector(tokenized_tweet_list_a, num_vocab=20000, padded=False)\n",
    "documents_vector_a = preprocess.get_vector_from_vocabulary(vocabulary, tokenized_tweet_list_a)\n",
    "\n",
    "sub_a_label_test = task_a_test_label['labela'].map(label2id).to_list()\n",
    "test_a_dataset = TextDataset(documents_vector_a, sub_a_label_test)\n",
    "test_a_loader = DataLoader(test_a_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    pred, label = [],[]\n",
    "    for i, (inputs, labels, lengths) in enumerate(test_a_loader):\n",
    "        outputs = model(inputs.long(), lengths)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        pred.append(predicted)\n",
    "        label.append(labels)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "y_pred = np.asarray(torch.cat(pred))\n",
    "y_label = np.asarray(torch.cat(label))\n",
    "print(metrics.f1_score(y_label, y_pred, average='micro'))\n",
    "print(metrics.classification_report(y_label, y_pred, target_names=['NOT', 'OFF']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10335, 25])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_b_idx = list(ori_train_data[ori_train_data['subtask_b'].notna()].index)\n",
    "tweet_list = ori_train_data.loc[sub_b_idx]['tweet'].to_list()\n",
    "processed_tweet_list = [preprocess.glove_twitter_preprocess(t) for t in tweet_list]\n",
    "tokenized_tweet_list = preprocess.nltk_tokenize(processed_tweet_list)\n",
    "vocabulary, documents_vector = preprocess.transform_word_to_vector(tokenized_tweet_list, num_vocab=20000, padded=False)\n",
    "pretrained_embedding = preprocess.get_embedding_from_torch_text(vocabulary, \"twitter.27B\", 25)\n",
    "pretrained_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {'UNT': 0, 'TIN': 1}\n",
    "sub_b_label = ori_train_data.loc[sub_b_idx]['subtask_b'].map(label2id).to_list()\n",
    "sentences_train, sentences_test, labels_train, labels_test = train_test_split(documents_vector, sub_b_label, test_size=0.2, random_state=5246)\n",
    "y_train = np.asarray(labels_train)\n",
    "y_test = np.asarray(labels_test)\n",
    "\n",
    "count_unt, count_tin= np.sum(y_train == 0), np.sum(y_train == 1)\n",
    "weight_unt = len(y_train) / (2 * count_unt)\n",
    "weight_tin = len(y_train) / (2 * count_tin)\n",
    "class_weights = torch.FloatTensor([weight_unt, weight_tin])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/110], Loss: 0.6551\n",
      "Epoch [2/10], Step [100/110], Loss: 0.4863\n",
      "Epoch [3/10], Step [100/110], Loss: 0.7053\n",
      "Epoch [4/10], Step [100/110], Loss: 0.5995\n",
      "Epoch [5/10], Step [100/110], Loss: 0.6077\n",
      "Epoch [6/10], Step [100/110], Loss: 0.5809\n",
      "Epoch [7/10], Step [100/110], Loss: 0.5817\n",
      "Epoch [8/10], Step [100/110], Loss: 0.6481\n",
      "Epoch [9/10], Step [100/110], Loss: 0.4105\n",
      "Epoch [10/10], Step [100/110], Loss: 0.3881\n",
      "Test Accuracy: 75.80%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         UNT       0.27      0.49      0.35       117\n",
      "         TIN       0.91      0.80      0.85       763\n",
      "\n",
      "    accuracy                           0.76       880\n",
      "   macro avg       0.59      0.64      0.60       880\n",
      "weighted avg       0.83      0.76      0.78       880\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = pretrained_embedding.size(1)\n",
    "hidden_dim = 64\n",
    "output_dim = 2\n",
    "lr = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "model = LSTMClassifier(embedding_dim, hidden_dim, output_dim, pretrained_embedding)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-3)\n",
    "\n",
    "train_dataset = TextDataset(sentences_train, y_train)\n",
    "test_dataset = TextDataset(sentences_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels, lengths) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs.long(), lengths)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    pred, label = [],[]\n",
    "    for i, (inputs, labels, lengths) in enumerate(test_loader):\n",
    "        outputs = model(inputs.long(), lengths)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        pred.append(predicted)\n",
    "        label.append(labels)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "y_pred = np.asarray(torch.cat(pred))\n",
    "y_label = np.asarray(torch.cat(label))\n",
    "print(metrics.classification_report(y_label, y_pred, target_names=['UNT', 'TIN']))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test task b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 41.25%\n",
      "0.4125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         UNT       0.14      0.85      0.25        27\n",
      "         TIN       0.95      0.36      0.52       213\n",
      "\n",
      "    accuracy                           0.41       240\n",
      "   macro avg       0.55      0.60      0.38       240\n",
      "weighted avg       0.86      0.41      0.49       240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_test_b = task_b_test_data[\"tweet\"].tolist()\n",
    "processed_tweet_list_b = [preprocess.glove_twitter_preprocess(t) for t in tweets_test_b]\n",
    "tokenized_tweet_list_b = preprocess.nltk_tokenize(processed_tweet_list_b)\n",
    "documents_vector_b = preprocess.get_vector_from_vocabulary(vocabulary, tokenized_tweet_list_b)\n",
    "\n",
    "sub_b_label_test = task_b_test_label['labelb'].map(label2id).to_list()\n",
    "test_b_dataset = TextDataset(documents_vector_b, sub_b_label_test)\n",
    "test_b_loader = DataLoader(test_b_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    pred, label = [],[]\n",
    "    for i, (inputs, labels, lengths) in enumerate(test_b_loader):\n",
    "        outputs = model(inputs.long(), lengths)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        pred.append(predicted)\n",
    "        label.append(labels)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "y_pred = np.asarray(torch.cat(pred))\n",
    "y_label = np.asarray(torch.cat(label))\n",
    "print(metrics.f1_score(y_label, y_pred, average='micro'))\n",
    "print(metrics.classification_report(y_label, y_pred, target_names=['UNT', 'TIN']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9679, 25])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_c_idx = list(ori_train_data[ori_train_data['subtask_c'].notna()].index)\n",
    "tweet_list = ori_train_data.loc[sub_c_idx]['tweet'].to_list()\n",
    "processed_tweet_list = [preprocess.glove_twitter_preprocess(t) for t in tweet_list]\n",
    "tokenized_tweet_list = preprocess.nltk_tokenize(processed_tweet_list)\n",
    "vocabulary, documents_vector = preprocess.transform_word_to_vector(tokenized_tweet_list, num_vocab=20000, padded=False)\n",
    "pretrained_embedding = preprocess.get_embedding_from_torch_text(vocabulary, \"twitter.27B\", 25)\n",
    "pretrained_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {'IND': 0, 'GRP': 1, 'OTH': 2}\n",
    "sub_c_label = ori_train_data.loc[sub_c_idx]['subtask_c'].map(label2id).to_list()\n",
    "sentences_train, sentences_test, labels_train, labels_test = train_test_split(documents_vector, sub_c_label, test_size=0.2, random_state=5246)\n",
    "y_train = np.asarray(labels_train)\n",
    "y_test = np.asarray(labels_test)\n",
    "\n",
    "count_ind, count_grp, count_oth = np.sum(y_train == 0), np.sum(y_train == 1), np.sum(y_train == 1)\n",
    "weight_ind = len(y_train) / (3 * count_ind)\n",
    "weight_grp = len(y_train) / (3 * count_grp)\n",
    "weight_oth = len(y_train) / (3 * count_oth)\n",
    "class_weights = torch.FloatTensor([weight_ind, weight_grp, weight_oth])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [50/97], Loss: 0.8945\n",
      "Epoch [2/10], Step [50/97], Loss: 0.8534\n",
      "Epoch [3/10], Step [50/97], Loss: 0.8560\n",
      "Epoch [4/10], Step [50/97], Loss: 1.0413\n",
      "Epoch [5/10], Step [50/97], Loss: 0.7240\n",
      "Epoch [6/10], Step [50/97], Loss: 0.9235\n",
      "Epoch [7/10], Step [50/97], Loss: 0.8876\n",
      "Epoch [8/10], Step [50/97], Loss: 0.9720\n",
      "Epoch [9/10], Step [50/97], Loss: 0.9349\n",
      "Epoch [10/10], Step [50/97], Loss: 0.8611\n",
      "Test Accuracy: 72.81%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         IND       0.84      0.82      0.83       500\n",
      "         GRP       0.57      0.71      0.63       203\n",
      "         OTH       0.30      0.12      0.17        73\n",
      "\n",
      "    accuracy                           0.73       776\n",
      "   macro avg       0.57      0.55      0.55       776\n",
      "weighted avg       0.72      0.73      0.72       776\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = pretrained_embedding.size(1)\n",
    "hidden_dim = 64\n",
    "output_dim = 3\n",
    "lr = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "model = LSTMClassifier(embedding_dim, hidden_dim, output_dim, pretrained_embedding)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-3)\n",
    "\n",
    "train_dataset = TextDataset(sentences_train, y_train)\n",
    "test_dataset = TextDataset(sentences_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels, lengths) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs.long(), lengths)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    pred, label = [],[]\n",
    "    for i, (inputs, labels, lengths) in enumerate(test_loader):\n",
    "        outputs = model(inputs.long(), lengths)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        pred.append(predicted)\n",
    "        label.append(labels)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "y_pred = np.asarray(torch.cat(pred))\n",
    "y_label = np.asarray(torch.cat(label))\n",
    "print(metrics.classification_report(y_label, y_pred, target_names=['IND', 'GRP', 'OTH']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 60.09%\n",
      "0.6009389671361502\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         IND       0.73      0.62      0.67       100\n",
      "         GRP       0.55      0.78      0.65        78\n",
      "         OTH       0.29      0.14      0.19        35\n",
      "\n",
      "    accuracy                           0.60       213\n",
      "   macro avg       0.52      0.51      0.50       213\n",
      "weighted avg       0.59      0.60      0.58       213\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_test_c = task_c_test_data[\"tweet\"].tolist()\n",
    "processed_tweet_list_c = [preprocess.glove_twitter_preprocess(t) for t in tweets_test_c]\n",
    "tokenized_tweet_list_c = preprocess.nltk_tokenize(processed_tweet_list_c)\n",
    "documents_vector_c = preprocess.get_vector_from_vocabulary(vocabulary, tokenized_tweet_list_c)\n",
    "\n",
    "sub_c_label_test = task_c_test_label['labelc'].map(label2id).to_list()\n",
    "test_c_dataset = TextDataset(documents_vector_c, sub_c_label_test)\n",
    "test_c_loader = DataLoader(test_c_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    pred, label = [],[]\n",
    "    for i, (inputs, labels, lengths) in enumerate(test_c_loader):\n",
    "        outputs = model(inputs.long(), lengths)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        pred.append(predicted)\n",
    "        label.append(labels)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "y_pred = np.asarray(torch.cat(pred))\n",
    "y_label = np.asarray(torch.cat(label))\n",
    "print(metrics.f1_score(y_label, y_pred, average='micro'))\n",
    "print(metrics.classification_report(y_label, y_pred, target_names=['IND', 'GRP', 'OTH']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of OOV: 2791\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<PAD>',\n",
       " '<UNK>',\n",
       " '<SOS>',\n",
       " '<EOS>',\n",
       " \"don't\",\n",
       " '😂',\n",
       " \"it's\",\n",
       " '🇺',\n",
       " '🇸',\n",
       " '️',\n",
       " 'kavanaugh',\n",
       " \"i'm\",\n",
       " \"that's\",\n",
       " \"can't\",\n",
       " \"you're\",\n",
       " '🤣',\n",
       " \"doesn't\",\n",
       " \"he's\",\n",
       " \"didn't\",\n",
       " '😭',\n",
       " \"isn't\",\n",
       " \"they're\",\n",
       " 'brexit',\n",
       " '🤔',\n",
       " \"she's\",\n",
       " '😍',\n",
       " '👍',\n",
       " \"won't\",\n",
       " '🙄',\n",
       " \"aren't\",\n",
       " '😘',\n",
       " \"i've\",\n",
       " \"there's\",\n",
       " \"i'll\",\n",
       " '😡',\n",
       " \"wouldn't\",\n",
       " \"let's\",\n",
       " \"what's\",\n",
       " '🙏',\n",
       " '🔥',\n",
       " '💕',\n",
       " \"wasn't\",\n",
       " '😊',\n",
       " '😁',\n",
       " '👏',\n",
       " '😉',\n",
       " 'qanon',\n",
       " '😆',\n",
       " '😀',\n",
       " \"we're\",\n",
       " '💜',\n",
       " \"i'd\",\n",
       " '💀',\n",
       " '😎',\n",
       " '💯',\n",
       " \"trump's\",\n",
       " \"who's\",\n",
       " '😠',\n",
       " \"haven't\",\n",
       " 'blasey',\n",
       " '💥',\n",
       " '💙',\n",
       " \"you'll\",\n",
       " \"you've\",\n",
       " '🌹',\n",
       " \"couldn't\",\n",
       " '💖',\n",
       " \"he'll\",\n",
       " 'mbga',\n",
       " '😢',\n",
       " \"shouldn't\",\n",
       " '😩',\n",
       " '🤢',\n",
       " '👊',\n",
       " \"women's\",\n",
       " '👌',\n",
       " 'alt-right',\n",
       " '🛑',\n",
       " '😒',\n",
       " '💩',\n",
       " '. . .',\n",
       " '👀',\n",
       " \"hasn't\",\n",
       " '‼',\n",
       " '🙏🏻',\n",
       " '💞',\n",
       " '👇',\n",
       " '😜',\n",
       " '. .',\n",
       " '😅',\n",
       " '🇷',\n",
       " '🇬',\n",
       " \"ain't\",\n",
       " '🤗',\n",
       " 'mcga',\n",
       " '<<number>',\n",
       " '🤡',\n",
       " '🗽',\n",
       " '👏🏻',\n",
       " '🇧',\n",
       " '😳',\n",
       " \"y'all\",\n",
       " \"people's\",\n",
       " '😱',\n",
       " \"weren't\",\n",
       " '💛',\n",
       " '👍🏼',\n",
       " '🙏🏼',\n",
       " \"we've\",\n",
       " '🐇',\n",
       " \"kavanaugh's\",\n",
       " '😋',\n",
       " 'declassify',\n",
       " \"he'd\",\n",
       " '🙂',\n",
       " \"we'll\",\n",
       " '🤮',\n",
       " '👇🏼',\n",
       " '🤷\\u200d♂',\n",
       " '🤨',\n",
       " \"here's\",\n",
       " '👉',\n",
       " '￼',\n",
       " \"woman's\",\n",
       " '😏',\n",
       " '🤦🏾\\u200d♂',\n",
       " '🇱',\n",
       " '😄',\n",
       " '👏🏼',\n",
       " 'manafort',\n",
       " '🤥',\n",
       " '💪',\n",
       " '💰',\n",
       " \"today's\",\n",
       " '🤪',\n",
       " '🇨',\n",
       " '🌊',\n",
       " \"they'll\",\n",
       " 'alleslev',\n",
       " '😤',\n",
       " '💓',\n",
       " \"they'd\",\n",
       " '<treph>',\n",
       " '🎶',\n",
       " '💚',\n",
       " 'deepstate',\n",
       " '🌪',\n",
       " '😇',\n",
       " \"they've\",\n",
       " '😃',\n",
       " '👋',\n",
       " '💪🏻',\n",
       " 'brexiteers',\n",
       " '🚂',\n",
       " '💔',\n",
       " '👑',\n",
       " '💗',\n",
       " '😥',\n",
       " \"man's\",\n",
       " \"god's\",\n",
       " '👈',\n",
       " '😞',\n",
       " '🖕🏼',\n",
       " \"should've\",\n",
       " \"america's\",\n",
       " \"hadn't\",\n",
       " '🇵',\n",
       " 'kavenaugh',\n",
       " '🤷\\u200d♀',\n",
       " '🙈',\n",
       " 'remainers',\n",
       " '👎',\n",
       " '👊🏻',\n",
       " '😈',\n",
       " '🧐',\n",
       " '🐾',\n",
       " \"gov't\",\n",
       " '🤷🏻\\u200d♀',\n",
       " 'anti-trump',\n",
       " \"president's\",\n",
       " '🇦',\n",
       " \"obama's\",\n",
       " '🏘',\n",
       " 'theresa_may',\n",
       " '🤦🏽\\u200d♀',\n",
       " 'scalise',\n",
       " '😬',\n",
       " '👆',\n",
       " '😹',\n",
       " '.  .',\n",
       " '🐷',\n",
       " '🖤',\n",
       " '👹',\n",
       " 'trumptrain',\n",
       " '🙉',\n",
       " 'politicslive',\n",
       " '👍🏻',\n",
       " '😗',\n",
       " '🤦🏻\\u200d♀',\n",
       " 'kavanaughs',\n",
       " 'servatives',\n",
       " \"antifa's\",\n",
       " 'brexitshambles',\n",
       " \"where's\",\n",
       " 'demonrats',\n",
       " \"one's\",\n",
       " '🙌',\n",
       " 'swalwell',\n",
       " '🥀',\n",
       " '🤬',\n",
       " '🤦\\u200d♀',\n",
       " 'fortnite',\n",
       " \"ford's\",\n",
       " '💐',\n",
       " 'alt-left',\n",
       " 'pump-action',\n",
       " \"who've\",\n",
       " \"feinstein's\",\n",
       " \"r's\",\n",
       " '🤫',\n",
       " \"levi's\",\n",
       " \"everyone's\",\n",
       " '😐',\n",
       " '😮',\n",
       " \"how's\",\n",
       " \"ellison's\",\n",
       " '🤷🏾\\u200d♂',\n",
       " '🖕',\n",
       " '🎉',\n",
       " '🤦\\u200d♂',\n",
       " 'draintheswamp',\n",
       " '🤷🏻\\u200d♂',\n",
       " '🤤',\n",
       " '🦅',\n",
       " '👎🏼',\n",
       " '🦇',\n",
       " \"men's\",\n",
       " 'windrush',\n",
       " '😌',\n",
       " '😑',\n",
       " 'dixiecrats',\n",
       " \"mom's\",\n",
       " '😛',\n",
       " 'demoncraps',\n",
       " \"she'll\",\n",
       " '😪',\n",
       " \"nation's\",\n",
       " 'rosenstein',\n",
       " 'rimmy',\n",
       " '📣',\n",
       " 'antiamerican',\n",
       " '🐍',\n",
       " '🤐',\n",
       " \"would've\",\n",
       " '🤦🏽\\u200d♂',\n",
       " 'leftest',\n",
       " 'everytown',\n",
       " '\\u2066',\n",
       " 'centrists',\n",
       " '🌚',\n",
       " '🤷🏼\\u200d♀',\n",
       " 'trumpers',\n",
       " '🦁',\n",
       " 'sarsour',\n",
       " 'avenatti',\n",
       " '🤭',\n",
       " 'kanavaugh',\n",
       " '🐊',\n",
       " 'iberals',\n",
       " '🙁',\n",
       " 'doxing',\n",
       " '😧',\n",
       " 'qarmy',\n",
       " 'sbornwomen',\n",
       " 'antifascist',\n",
       " 'talaq',\n",
       " \"israel's\",\n",
       " ':p',\n",
       " '🙏🏾',\n",
       " 'truthfeed',\n",
       " '🤷🏽\\u200d♂',\n",
       " 'sargon',\n",
       " '🍄',\n",
       " 'onevoice',\n",
       " '😫',\n",
       " 'lynchings',\n",
       " '🍎',\n",
       " '🇪',\n",
       " '💡',\n",
       " 'anti-fascism',\n",
       " 'trumpsters',\n",
       " '🤦🏼\\u200d♂',\n",
       " '😕',\n",
       " '💨',\n",
       " 'kogasa',\n",
       " \"anyone's\",\n",
       " '🧡',\n",
       " '💃',\n",
       " '🙃',\n",
       " 'ashwariya',\n",
       " 'demonrat',\n",
       " '💘',\n",
       " 'redwave',\n",
       " 'cdnhist',\n",
       " 'americafirst',\n",
       " \"labour's\",\n",
       " '🤠',\n",
       " \"fascist's\",\n",
       " 'ex-crackheads',\n",
       " '🤟',\n",
       " 'greatawakening',\n",
       " 'declass',\n",
       " '😶',\n",
       " \"party's\",\n",
       " 'murkowski',\n",
       " 'poltics',\n",
       " \"canada's\",\n",
       " \"else's\",\n",
       " 'krishn',\n",
       " 'marginalizing',\n",
       " 'kalergi',\n",
       " 'blocklists',\n",
       " 'blocktogether',\n",
       " '💝',\n",
       " '🌸',\n",
       " 'back-door',\n",
       " 'nservatives',\n",
       " '😣',\n",
       " '🐕',\n",
       " 'wwgiwga',\n",
       " '👊🏽',\n",
       " '⁉',\n",
       " 'antideutsch',\n",
       " 'fashy',\n",
       " 'anti-antifa',\n",
       " \"corbyn's\",\n",
       " 'ntknetwork',\n",
       " '🙋',\n",
       " '💋',\n",
       " 'yuge',\n",
       " '🇳',\n",
       " '🤦🏾\\u200d♀',\n",
       " 'self-professed',\n",
       " 'stickerwoman',\n",
       " 'indoctrinating',\n",
       " 'friendsandfamily',\n",
       " '🤒',\n",
       " '--->',\n",
       " 'antifa-types',\n",
       " 'trumper',\n",
       " '🤷🏼\\u200d♂',\n",
       " 'pittis',\n",
       " '🌍',\n",
       " 'perjurer',\n",
       " 'atsushi-kun',\n",
       " '🤷🏾\\u200d♀',\n",
       " '🤯',\n",
       " '🗨',\n",
       " 'stranahahn',\n",
       " \"left's\",\n",
       " 'disarmthem',\n",
       " \"yesterday's\",\n",
       " \"mother's\",\n",
       " '🏳',\n",
       " '️\\u200d🌈',\n",
       " \"k's\",\n",
       " \"dad's\",\n",
       " 'jesuslives',\n",
       " '🤧',\n",
       " 'pugchat',\n",
       " 'dubakoor',\n",
       " 'remainer',\n",
       " 'supersoldiers',\n",
       " 'qalert',\n",
       " 'magaforallinc',\n",
       " 'doubloons',\n",
       " 'swomen',\n",
       " 'tziyon',\n",
       " \"it'll\",\n",
       " 'trumpism',\n",
       " '🤑',\n",
       " 'perjure',\n",
       " \"you'd\",\n",
       " 'gunbroker',\n",
       " '🐶',\n",
       " 'swedish-style',\n",
       " 'gillum',\n",
       " 'sturmabteilung',\n",
       " 'near-universal',\n",
       " '📖',\n",
       " '👐',\n",
       " '👼',\n",
       " \"bono's\",\n",
       " '🌷',\n",
       " 'victim-hood',\n",
       " 'semite',\n",
       " 'drellenbrandt',\n",
       " '🌈',\n",
       " 'substantiated',\n",
       " '😨',\n",
       " \"someone's\",\n",
       " '🥂',\n",
       " 'followthewhiterabbit',\n",
       " 'wasr',\n",
       " 'illegalalien',\n",
       " 'sitroom',\n",
       " 'snewshour',\n",
       " '》',\n",
       " '🐈',\n",
       " 'cillizza',\n",
       " 'fascists-in-training',\n",
       " 'corralled',\n",
       " 'me-too',\n",
       " 'cotus',\n",
       " \"nazi's\",\n",
       " 'pristin',\n",
       " 'anti-kavanaugh',\n",
       " '💵',\n",
       " 'votered',\n",
       " \"a's\",\n",
       " 'tryants',\n",
       " 'happyhyunjaeday',\n",
       " 'believe_hyunjae_day',\n",
       " '너의존재가_기적이야',\n",
       " 'drumpf',\n",
       " 'bhuvan',\n",
       " \"moore's\",\n",
       " '💴',\n",
       " '🚫',\n",
       " \"ya'll\",\n",
       " 'torb',\n",
       " 'nguni',\n",
       " 'mosotho',\n",
       " \"group's\",\n",
       " 'reppowervote',\n",
       " 'powerprayers',\n",
       " 'anti-german',\n",
       " 'anti-americans',\n",
       " 'demorats',\n",
       " 'ringwraith',\n",
       " 'bhagwad',\n",
       " 'semetic',\n",
       " '👌🏼',\n",
       " '📺',\n",
       " \"it'd\",\n",
       " 'rules-based',\n",
       " 'cnco',\n",
       " 'pro-trump',\n",
       " 'doxx',\n",
       " 'trump_babes',\n",
       " 'trumpbabes',\n",
       " 'babesfortrump',\n",
       " 'bigly',\n",
       " 'fakedemocracy',\n",
       " 'cddr',\n",
       " '🎵',\n",
       " '✊🏾',\n",
       " \"she'd\",\n",
       " \"pedo's\",\n",
       " 'degrom',\n",
       " 'fbpe',\n",
       " 'sinnfeinn',\n",
       " '🖕🏽',\n",
       " '🧟\\u200d♂',\n",
       " 'handmaids',\n",
       " 'glbl',\n",
       " 'beeching',\n",
       " 'denuclearization',\n",
       " 'allsup',\n",
       " '😻',\n",
       " '💦',\n",
       " '😔',\n",
       " \"trudeau's\",\n",
       " \"ontario's\",\n",
       " 'kneelers',\n",
       " 'a_',\n",
       " '🏈',\n",
       " 'luddites',\n",
       " 'kabasele',\n",
       " \"watford's\",\n",
       " '🐸',\n",
       " 'brexiteer',\n",
       " 'curtailing',\n",
       " '😲',\n",
       " 'assulting',\n",
       " \"hitler's\",\n",
       " 'guarantied',\n",
       " 'teamspotlightsunday',\n",
       " '.   .',\n",
       " 'guttenburg',\n",
       " 'trumpstrong',\n",
       " '🐑',\n",
       " 'no-deal',\n",
       " 'strzok',\n",
       " 'morphe',\n",
       " 'law.obviously',\n",
       " 'pussyhats',\n",
       " 'lbclive',\n",
       " 'cityoflondon',\n",
       " 'toryparty',\n",
       " 'dimms',\n",
       " \"children's\",\n",
       " '🇻',\n",
       " '🤖',\n",
       " 'cristyn',\n",
       " 'conscientiousness',\n",
       " 'demoncratic',\n",
       " '🙌🏻',\n",
       " \"clinton's\",\n",
       " \"amendment's\",\n",
       " 'ultrafinitists',\n",
       " 'declassification',\n",
       " '🤞',\n",
       " '✊🏿',\n",
       " 'ddsl',\n",
       " 'ginsbergisnext',\n",
       " 'bodyshaming',\n",
       " 'biogenic',\n",
       " \"person's\",\n",
       " 'vigano',\n",
       " '🔮',\n",
       " 'kavanauh',\n",
       " 'mooslims',\n",
       " \"should'vetaken\",\n",
       " 'qproofs',\n",
       " 'demsare',\n",
       " 'denuke',\n",
       " 'demsuck',\n",
       " 'alloutpolitics',\n",
       " 'ethru',\n",
       " 'theusa',\n",
       " 'geonicidal',\n",
       " 'toxictory',\n",
       " 'toriesmustgo',\n",
       " 'dewwane',\n",
       " 'porrany',\n",
       " 'manmarziyaan',\n",
       " 'alinskyite',\n",
       " \"d'souza's\",\n",
       " 'antifa-haters',\n",
       " 'hypocrickets',\n",
       " 'nevinbruce',\n",
       " 'communist-socialist-nazis',\n",
       " 'ruskies',\n",
       " 'partyvery',\n",
       " 'sjshsj',\n",
       " 'dumocrat',\n",
       " 'farakans',\n",
       " 'hilliary',\n",
       " 'sturmer',\n",
       " 'einsatzgruppen',\n",
       " 'gerrymandered',\n",
       " 'fonding',\n",
       " 'ocasio-cortez',\n",
       " 'summer-fall',\n",
       " 'winter-spring',\n",
       " 'self-devaluation',\n",
       " 'facsists',\n",
       " 'inputed',\n",
       " 'capablities',\n",
       " \"follower's\",\n",
       " 'languase',\n",
       " 'neanderthals.would',\n",
       " 'contemptuously',\n",
       " 'turmps',\n",
       " \"ball's\",\n",
       " 'terroism',\n",
       " '𝙿𝚎𝚝𝚊𝚕𝚜',\n",
       " '𝙼𝚌𝙿𝙾𝚄𝚃𝚂𝙵𝚊𝚌𝚎',\n",
       " 'vainly',\n",
       " 'meme-ing',\n",
       " 'christineford',\n",
       " '🤷',\n",
       " 'america-hating',\n",
       " 'ex-dems',\n",
       " 'machinelearning',\n",
       " 'jeezuz.fucking.christ',\n",
       " 'diásporaenresistencia',\n",
       " \"was't\",\n",
       " 'premediated',\n",
       " 'control.they',\n",
       " '✌🏼',\n",
       " '👮🏻',\n",
       " '👩🏻\\u200d✈',\n",
       " 'teacherlife',\n",
       " 'prayalloftheprayers',\n",
       " 'salinsky',\n",
       " 'privertised',\n",
       " 'non-voters',\n",
       " \"community's\",\n",
       " \"somebody's\",\n",
       " 'leavem',\n",
       " 'ajsjjsjdkkdjdk',\n",
       " 'characterizations',\n",
       " 'baiters',\n",
       " 'oshaeterry',\n",
       " \"may's\",\n",
       " 'signifing',\n",
       " 'mini-g',\n",
       " 'donybrooke',\n",
       " 'nutbags',\n",
       " 'globalism',\n",
       " 'slave-owners',\n",
       " 'renacci',\n",
       " 'batsht',\n",
       " 'fordnation',\n",
       " 'aesir',\n",
       " 'vanir',\n",
       " \"bloomberg's\",\n",
       " 'yesssssh',\n",
       " 'shitgull',\n",
       " '😚',\n",
       " 'moonves',\n",
       " 'iannello',\n",
       " 'srocks',\n",
       " '🙌🏾',\n",
       " \"nike's\",\n",
       " '️for',\n",
       " 'ehave',\n",
       " '🙌🏽',\n",
       " '👏🏽',\n",
       " 'breibart',\n",
       " 'inauthenticity',\n",
       " \"woodword's\",\n",
       " 'told.its',\n",
       " 'rethuglicans',\n",
       " 'liberallosers',\n",
       " 'trumpman',\n",
       " 'crminal',\n",
       " 'heroineics',\n",
       " 'loosley',\n",
       " 'dtsunami',\n",
       " 'truth-teller',\n",
       " 'schmata',\n",
       " 'papercup',\n",
       " '🎂',\n",
       " 'agentof',\n",
       " 'chaurch',\n",
       " 'razzinfrazzinmaggle',\n",
       " 'mybubblygirl',\n",
       " 'anupama',\n",
       " \"ted's\",\n",
       " 'writerwednesday',\n",
       " \"lie'n\",\n",
       " 'oooohohohohohoh',\n",
       " 'nbcct',\n",
       " 'wdym',\n",
       " 'croocked',\n",
       " 'taklked',\n",
       " 'privitizing',\n",
       " 'multiversity',\n",
       " 'fordfestnd',\n",
       " \"ryan's\",\n",
       " 'balaam',\n",
       " 'cosnstitution',\n",
       " 'totalitarain',\n",
       " 'civically',\n",
       " 'kanaugh',\n",
       " 'desanctis',\n",
       " 'beaboutit',\n",
       " 'jesuiten',\n",
       " 'expropriate',\n",
       " '🕳',\n",
       " 'fjshdhjs',\n",
       " 'doomentio',\n",
       " 'crookedmedia',\n",
       " 'non-victims',\n",
       " 'supremecists',\n",
       " 'lvmpd',\n",
       " 'covention',\n",
       " \"madcow's\",\n",
       " 'nipigon',\n",
       " 'plelleeaaaase',\n",
       " 'sleeeeeeeps',\n",
       " 'leftwingers',\n",
       " 'd.org',\n",
       " 'proletarian',\n",
       " 'administration.can',\n",
       " 'either.still',\n",
       " 'rights.hoping',\n",
       " '𝓭𝔂𝓼𝓹𝓱𝓸𝓻𝓲𝓪',\n",
       " \"jean's\",\n",
       " '⛏',\n",
       " 'jonso',\n",
       " 'kinew',\n",
       " 'bpoli',\n",
       " 'pro-law',\n",
       " 'eddandflow',\n",
       " 'credible-keith',\n",
       " 'career-embarrass',\n",
       " 'daughters-all',\n",
       " 'killshot',\n",
       " 'karyakarta',\n",
       " 'pacified',\n",
       " 'hadwin',\n",
       " \"steele's\",\n",
       " 'nemzet',\n",
       " 'népszabadság',\n",
       " 'communistic',\n",
       " 'laughts',\n",
       " 'breaked',\n",
       " '📱',\n",
       " 'agholor',\n",
       " 'atempted',\n",
       " 'fleeted',\n",
       " \"criminal's\",\n",
       " 'joebiden',\n",
       " '🐨',\n",
       " '🐻',\n",
       " '_against',\n",
       " 'jimin-ssi',\n",
       " 'pro-žižek',\n",
       " 'leftypol',\n",
       " 'fabricates',\n",
       " 'condemnations',\n",
       " 'false-majority',\n",
       " 'antifa-lite',\n",
       " 'a-crooked-letter-crooked-letter',\n",
       " 'baby-adults',\n",
       " \"pussy's\",\n",
       " 'kdksksks',\n",
       " 'nessasary',\n",
       " 'tacozt',\n",
       " \"kill's\",\n",
       " \"dumpster's\",\n",
       " \"toddler's\",\n",
       " \"kyle's\",\n",
       " '🎁',\n",
       " \"dinner's\",\n",
       " 'phinnaly',\n",
       " 'conservatards',\n",
       " \"country's\",\n",
       " 'putin-puppet',\n",
       " 'sugardaddyneeded',\n",
       " 'relativizing',\n",
       " '️ps',\n",
       " 'tweeeeeetr',\n",
       " 'whaooa',\n",
       " 'fessy',\n",
       " \"tyler's\",\n",
       " 'anti-pernickety',\n",
       " 'hadex',\n",
       " \"devil's\",\n",
       " 'rulesfor',\n",
       " 'populists',\n",
       " 'momsdemand',\n",
       " 'izlam',\n",
       " 'donaldtrumprallies',\n",
       " 'liar.they',\n",
       " 'member.insane',\n",
       " 'yrs.if',\n",
       " 'complancey',\n",
       " 'againstbthe',\n",
       " 'temper-tantrums',\n",
       " 'beto-male',\n",
       " 'rudeand',\n",
       " 'anti-freedom',\n",
       " 'loomer',\n",
       " 'tectics',\n",
       " 'tikoo',\n",
       " '😴',\n",
       " 'redto',\n",
       " 'fluctuat',\n",
       " 'mergitur',\n",
       " 'shepell',\n",
       " \"socialist's\",\n",
       " 'fanniemae',\n",
       " 'releasethetexts',\n",
       " 'redactions',\n",
       " 'obamagate',\n",
       " 'racism-bullshit',\n",
       " 'k-college',\n",
       " \"h'wood\",\n",
       " 'open-season',\n",
       " 'trumpists',\n",
       " 'evisceration',\n",
       " 'wisdomwednesday',\n",
       " 'opperman',\n",
       " 'knockmy',\n",
       " 'tellu',\n",
       " 'battlebus',\n",
       " 'generationally',\n",
       " 'on-it',\n",
       " 'bedfellow',\n",
       " 'maulis',\n",
       " 'nasks',\n",
       " 'carruing',\n",
       " 'naheru',\n",
       " 'shitbiscuit',\n",
       " '💼',\n",
       " '🕶',\n",
       " '👢',\n",
       " 'almostcousins',\n",
       " 'counterprotesters',\n",
       " 're-form',\n",
       " \"pig's\",\n",
       " \"son's\",\n",
       " 'walkawayfromdemocrats',\n",
       " 'halper',\n",
       " 'facelifted',\n",
       " 'ketterer',\n",
       " 'kettterer',\n",
       " 'tinderbox',\n",
       " 'hobag',\n",
       " 'takendted',\n",
       " 'dtmag',\n",
       " 'imbecild',\n",
       " 'anqueefa',\n",
       " '💉',\n",
       " 'weaponizing',\n",
       " 'fiberals',\n",
       " 'presedent',\n",
       " 'lowness',\n",
       " 'history.but',\n",
       " 'borders.doesn',\n",
       " 'bloodflies',\n",
       " 'resisters',\n",
       " 'allaying',\n",
       " 'probely',\n",
       " 'trumpies',\n",
       " 'bandwagonists',\n",
       " 'sleazballs',\n",
       " 'villifying',\n",
       " 'bolsheviks',\n",
       " 'somecunt',\n",
       " 'revkin',\n",
       " 'heughan',\n",
       " 'laser-focused',\n",
       " 'epithats',\n",
       " '👮\\u200d♀',\n",
       " 'bush-cheney',\n",
       " 'fuckingclass',\n",
       " 'ho-man',\n",
       " 'weeabo',\n",
       " 'extremistische',\n",
       " 'gag-inducing',\n",
       " \"everybody's\",\n",
       " 'schiffhead',\n",
       " 'examing',\n",
       " 'cherry-picked',\n",
       " 'hitlary',\n",
       " 'orcastrated',\n",
       " 'angeela',\n",
       " 'intitled',\n",
       " '💁🏽\\u200d♀',\n",
       " 'livingtrust',\n",
       " \"world's\",\n",
       " \"victim's\",\n",
       " 'villianized',\n",
       " 'sdlive',\n",
       " 'man.if',\n",
       " 'deleiver',\n",
       " 'adulting',\n",
       " 'illegalaliens',\n",
       " 'trumpwall',\n",
       " 'ocouncil',\n",
       " 'libtarded',\n",
       " 'marriedtomedicine',\n",
       " 'birdlandia',\n",
       " 'birdsize',\n",
       " \"serling's\",\n",
       " 'baselessly',\n",
       " 'korzemba',\n",
       " 'liberalling',\n",
       " 'onomy',\n",
       " 'stretchedface',\n",
       " 'mindmill',\n",
       " 'thescore',\n",
       " 'eotp',\n",
       " '️everyone',\n",
       " 'counselled',\n",
       " 'misdrawed',\n",
       " 'stcshooting',\n",
       " 'kneejerk',\n",
       " 'oh-no',\n",
       " 'un-indicted',\n",
       " 'co-conspirator',\n",
       " 'scjous',\n",
       " 'buildthewall',\n",
       " 'tanrums',\n",
       " 'acedemia',\n",
       " 'daxamite',\n",
       " 'america-phobes',\n",
       " 'well-fostered',\n",
       " 'revorded',\n",
       " 'griexsevia',\n",
       " 'records-having',\n",
       " 'go-so',\n",
       " 'govoner',\n",
       " 'sjsnsn',\n",
       " 'slent',\n",
       " 'dorter',\n",
       " 'alt-shite',\n",
       " 'foulmouthed',\n",
       " 'eighjt',\n",
       " 'dexades',\n",
       " \"partner's\",\n",
       " \"online's\",\n",
       " 'polloftheday',\n",
       " 'recusal',\n",
       " 'boypussy',\n",
       " 'nstein',\n",
       " '🐩',\n",
       " 'rf.then',\n",
       " 'island.latest',\n",
       " 'world.what',\n",
       " 'moviepass',\n",
       " \"needn't\",\n",
       " 'shurmer',\n",
       " 'carryons',\n",
       " '🆗',\n",
       " 'undisputable',\n",
       " 'hyper-partisan',\n",
       " 'ocasio',\n",
       " 'legislatively',\n",
       " 'wthin',\n",
       " 'unfuckable',\n",
       " 'money.liberals',\n",
       " 'trashno',\n",
       " 'warriorscoach',\n",
       " 'headcoach',\n",
       " 'damigo',\n",
       " 'brookee',\n",
       " 'antifagoons',\n",
       " 'fukushima-daiichi',\n",
       " '🤟🏼',\n",
       " 'one--it',\n",
       " 'complain--talk',\n",
       " 'twtich',\n",
       " '🌞',\n",
       " \"bush's\",\n",
       " 'nezha',\n",
       " 'nidus',\n",
       " 'colinjost',\n",
       " 'jiyoonfact',\n",
       " 'kuniva',\n",
       " 'bobbybrown',\n",
       " 'leolahbrown',\n",
       " 'celebnmusic',\n",
       " 'thebobbybrownstory',\n",
       " 'interesting-cause',\n",
       " 'us-but',\n",
       " 'tapdance',\n",
       " 'melkor',\n",
       " 'kavanah',\n",
       " 'girls.she',\n",
       " 'shootings-must',\n",
       " 'woker',\n",
       " 'shafticus',\n",
       " 'taravat',\n",
       " 'citizenshop',\n",
       " 'jeremy_hunt',\n",
       " '🇯',\n",
       " '🔊',\n",
       " 'workjams',\n",
       " 'treasons',\n",
       " 'junkees',\n",
       " \"hillary's\",\n",
       " 'parrishes',\n",
       " 'elizabethholmes',\n",
       " 'theranos',\n",
       " 'months-according',\n",
       " 'standadd',\n",
       " 'andriese',\n",
       " 'children.the',\n",
       " 'podcastful',\n",
       " 'blockchainful',\n",
       " \"it'sful\",\n",
       " 'meansful',\n",
       " 'mothersful',\n",
       " 'makewise',\n",
       " 'unindicted',\n",
       " 'co-conspirators',\n",
       " 'comygate',\n",
       " 'scholar-like',\n",
       " 'x.not',\n",
       " 'conservatorship',\n",
       " 'gungrabber',\n",
       " 'merkelkratie',\n",
       " '_proven_',\n",
       " 'brownshirt',\n",
       " 'less-than',\n",
       " 'toadette',\n",
       " 'nsmbu',\n",
       " 'everchanging',\n",
       " 'apists',\n",
       " 'congratulaions',\n",
       " \"swearer's\",\n",
       " \"lott's\",\n",
       " \"lancet's\",\n",
       " 'doxxing',\n",
       " 'mumtaj',\n",
       " 'loveforjess',\n",
       " \"that'll\",\n",
       " 'fake-republican',\n",
       " 'terrorsits',\n",
       " 'mbappe',\n",
       " 'today.few',\n",
       " \"ol'bob\",\n",
       " 'necular',\n",
       " ...]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeros_mask = torch.all(pretrained_embedding == 0, dim=1)\n",
    "zeros_indices = torch.nonzero(zeros_mask)\n",
    "print('Size of OOV: {}'.format(len(zeros_indices.squeeze())))\n",
    "[t for i, t in enumerate(vocabulary.get_itos()) if i in zeros_indices.squeeze()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d8b59bf2e713f17ff47b76154068196e0642fc6c62d3839f6eb91aeb6e90fd05"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
