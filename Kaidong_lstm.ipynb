{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from utils import preprocess, plotutils\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from spacy.lang.en import stop_words\n",
    "from string import punctuation\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86426</td>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90194</td>\n",
       "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16820</td>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62688</td>\n",
       "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43605</td>\n",
       "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet subtask_a  \\\n",
       "0  86426  @USER She should ask a few native Americans wh...       OFF   \n",
       "1  90194  @USER @USER Go home you’re drunk!!! @USER #MAG...       OFF   \n",
       "2  16820  Amazon is investigating Chinese employees who ...       NOT   \n",
       "3  62688  @USER Someone should'veTaken\" this piece of sh...       OFF   \n",
       "4  43605  @USER @USER Obama wanted liberals &amp; illega...       NOT   \n",
       "\n",
       "  subtask_b subtask_c  \n",
       "0       UNT       NaN  \n",
       "1       TIN       IND  \n",
       "2       NaN       NaN  \n",
       "3       UNT       NaN  \n",
       "4       NaN       NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = \"./data/\"\n",
    "TRAIN_DATA_FILE = DATA_DIR + \"olid-training-v1.0.tsv\"\n",
    "ori_train_data = pd.read_csv(TRAIN_DATA_FILE, sep='\\t')\n",
    "ori_train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_label(a, b, c):\n",
    "    if a == 'NOT':\n",
    "        return a\n",
    "    elif b == 'UNT':\n",
    "        return b\n",
    "    else:\n",
    "        return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_train_data['label'] = ori_train_data.apply(lambda x: compute_label(x.subtask_a, x.subtask_b, x.subtask_c), axis=1)\n",
    "count = ori_train_data.groupby('label').size().to_dict()\n",
    "stop_words = stop_words.STOP_WORDS\n",
    "stop_words = stop_words.union(set(emoji.get_emoji_unicode_dict('en').values()))\n",
    "stop_words.add(\"url\")\n",
    "stop_words = stop_words.union(set(punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_tokenization(doc):\n",
    "    tweet_tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, match_phone_numbers=False)\n",
    "    tweet_tokenize = [tweet_tokenizer.tokenize(tweet) for tweet in doc]\n",
    "    return tweet_tokenize\n",
    "\n",
    "def stop_word_removal(doc, stop_words):\n",
    "    new_tweets = []\n",
    "    for t in doc:\n",
    "        new_words = []\n",
    "        for word in t:\n",
    "            if word not in stop_words:\n",
    "                new_words.append(word)\n",
    "        new_tweets.append(new_words)\n",
    "\n",
    "    return new_tweets\n",
    "\n",
    "def hashtag_removal(doc):\n",
    "    new_docs = []\n",
    "    for t in doc:\n",
    "        new_doc=[]\n",
    "        for word in t:\n",
    "            if word[0] == \"#\":\n",
    "                new_doc.append(word[1:])\n",
    "            else:\n",
    "                new_doc.append(word)\n",
    "        new_docs.append(new_doc)\n",
    "    \n",
    "    return new_docs\n",
    "\n",
    "tweets = ori_train_data[\"tweet\"]\n",
    "tweet_tokenization = tweet_tokenization(tweets)\n",
    "tweet_remove_stop_word = stop_word_removal(tweet_tokenization, stop_words)\n",
    "tweets_remove = hashtag_removal(tweet_remove_stop_word)\n",
    "tweets_remove = [\" \".join(t) for t in tweets_remove]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: 10592\n",
      "Size of test set: 2648\n"
     ]
    }
   ],
   "source": [
    "label2id = {'NOT': 0, 'OFF': 1}\n",
    "sub_a_label = ori_train_data['subtask_a'].map(label2id).to_list()\n",
    "sentences_train, sentences_test, labels_train, labels_test = train_test_split(tweets_remove, sub_a_label, test_size=0.2, random_state=5246)\n",
    "\n",
    "y_train = np.asarray(labels_train)\n",
    "y_test = np.asarray(labels_test)\n",
    "\n",
    "print(\"Size of training set: {}\".format(len(sentences_train)))\n",
    "print(\"Size of test set: {}\".format(len(sentences_test)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/331], Loss: 0.7050\n",
      "Epoch [1/10], Step [200/331], Loss: 0.5693\n",
      "Epoch [1/10], Step [300/331], Loss: 0.4136\n",
      "Epoch [2/10], Step [100/331], Loss: 0.2369\n",
      "Epoch [2/10], Step [200/331], Loss: 0.1267\n",
      "Epoch [2/10], Step [300/331], Loss: 0.2413\n",
      "Epoch [3/10], Step [100/331], Loss: 0.1408\n",
      "Epoch [3/10], Step [200/331], Loss: 0.0646\n",
      "Epoch [3/10], Step [300/331], Loss: 0.0280\n",
      "Epoch [4/10], Step [100/331], Loss: 0.0337\n",
      "Epoch [4/10], Step [200/331], Loss: 0.0106\n",
      "Epoch [4/10], Step [300/331], Loss: 0.0150\n",
      "Epoch [5/10], Step [100/331], Loss: 0.0999\n",
      "Epoch [5/10], Step [200/331], Loss: 0.0177\n",
      "Epoch [5/10], Step [300/331], Loss: 0.0711\n",
      "Epoch [6/10], Step [100/331], Loss: 0.0022\n",
      "Epoch [6/10], Step [200/331], Loss: 0.0011\n",
      "Epoch [6/10], Step [300/331], Loss: 0.0768\n",
      "Epoch [7/10], Step [100/331], Loss: 0.0086\n",
      "Epoch [7/10], Step [200/331], Loss: 0.0053\n",
      "Epoch [7/10], Step [300/331], Loss: 0.0156\n",
      "Epoch [8/10], Step [100/331], Loss: 0.0016\n",
      "Epoch [8/10], Step [200/331], Loss: 0.0011\n",
      "Epoch [8/10], Step [300/331], Loss: 0.0305\n",
      "Epoch [9/10], Step [100/331], Loss: 0.0044\n",
      "Epoch [9/10], Step [200/331], Loss: 0.0004\n",
      "Epoch [9/10], Step [300/331], Loss: 0.0112\n",
      "Epoch [10/10], Step [100/331], Loss: 0.0179\n",
      "Epoch [10/10], Step [200/331], Loss: 0.0007\n",
      "Epoch [10/10], Step [300/331], Loss: 0.0032\n",
      "Test Accuracy: 72.32%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         NOT       0.76      0.84      0.80      1750\n",
      "         OFF       0.61      0.50      0.55       898\n",
      "\n",
      "    accuracy                           0.72      2648\n",
      "   macro avg       0.69      0.67      0.67      2648\n",
      "weighted avg       0.71      0.72      0.72      2648\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, vectorizer):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.vectorizer = vectorizer\n",
    "        self.sequences = self.vectorizer.transform(sentences).toarray()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sequences[idx]), torch.tensor(self.labels[idx])\n",
    "\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(1, batch_size, self.hidden_dim).requires_grad_()\n",
    "        c0 = torch.zeros(1, batch_size, self.hidden_dim).requires_grad_()\n",
    "\n",
    "        x = x.view(batch_size, 1, -1)\n",
    "\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        out = self.fc(hn[-1])\n",
    "        return out\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english', lowercase=True)\n",
    "X_train = vectorizer.fit_transform(sentences_train)\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dim = 100\n",
    "output_dim = 2\n",
    "lr = 0.01\n",
    "num_epochs = 10\n",
    "\n",
    "\n",
    "model = LSTMClassifier(input_dim, hidden_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "train_dataset = TextDataset(sentences_train, y_train, vectorizer)\n",
    "test_dataset = TextDataset(sentences_test, y_test, vectorizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    hn, cn = None, None\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        if hn is not None and cn is not None:\n",
    "            hn, cn = hn.detach(), cn.detach()\n",
    "        outputs = model(inputs.float())\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        hn, cn = outputs[1][0].detach(), outputs[1][1].detach()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "hn, cn = None, None\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    pred = []\n",
    "    for i, (inputs, labels) in enumerate(test_loader):\n",
    "\n",
    "        outputs = model(inputs.float())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        pred.append(predicted)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "y_pred = np.asarray(torch.cat(pred))\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=['NOT', 'OFF']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pack Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/331], Loss: 0.5411\n",
      "Epoch [1/10], Step [200/331], Loss: 0.5427\n",
      "Epoch [1/10], Step [300/331], Loss: 0.5888\n",
      "Epoch [2/10], Step [100/331], Loss: 0.3349\n",
      "Epoch [2/10], Step [200/331], Loss: 0.2287\n",
      "Epoch [2/10], Step [300/331], Loss: 0.3291\n",
      "Epoch [3/10], Step [100/331], Loss: 0.0798\n",
      "Epoch [3/10], Step [200/331], Loss: 0.0833\n",
      "Epoch [3/10], Step [300/331], Loss: 0.1384\n",
      "Epoch [4/10], Step [100/331], Loss: 0.0739\n",
      "Epoch [4/10], Step [200/331], Loss: 0.2047\n",
      "Epoch [4/10], Step [300/331], Loss: 0.0385\n",
      "Epoch [5/10], Step [100/331], Loss: 0.0079\n",
      "Epoch [5/10], Step [200/331], Loss: 0.0235\n",
      "Epoch [5/10], Step [300/331], Loss: 0.0422\n",
      "Epoch [6/10], Step [100/331], Loss: 0.0016\n",
      "Epoch [6/10], Step [200/331], Loss: 0.0065\n",
      "Epoch [6/10], Step [300/331], Loss: 0.0069\n",
      "Epoch [7/10], Step [100/331], Loss: 0.0019\n",
      "Epoch [7/10], Step [200/331], Loss: 0.3279\n",
      "Epoch [7/10], Step [300/331], Loss: 0.0037\n",
      "Epoch [8/10], Step [100/331], Loss: 0.0030\n",
      "Epoch [8/10], Step [200/331], Loss: 0.0015\n",
      "Epoch [8/10], Step [300/331], Loss: 0.0398\n",
      "Epoch [9/10], Step [100/331], Loss: 0.0020\n",
      "Epoch [9/10], Step [200/331], Loss: 0.0021\n",
      "Epoch [9/10], Step [300/331], Loss: 0.0002\n",
      "Epoch [10/10], Step [100/331], Loss: 0.0036\n",
      "Epoch [10/10], Step [200/331], Loss: 0.0081\n",
      "Epoch [10/10], Step [300/331], Loss: 0.0007\n",
      "Test Accuracy: 72.05%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         NOT       0.77      0.82      0.79      1750\n",
      "         OFF       0.60      0.53      0.56       898\n",
      "\n",
      "    accuracy                           0.72      2648\n",
      "   macro avg       0.69      0.67      0.68      2648\n",
      "weighted avg       0.71      0.72      0.72      2648\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(batch):\n",
    "    batch.sort(key=lambda x: x[2], reverse=True)\n",
    "    sequences, labels, lengths = zip(*batch)\n",
    "    sequences = nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=0.0)\n",
    "    return sequences, torch.tensor(labels), torch.tensor(lengths)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, vectorizer, input_dim):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.vectorizer = vectorizer\n",
    "        self.sequences = [torch.tensor(vectorizer.transform([sentence]).toarray()).reshape(-1, input_dim) for sentence in sentences]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], torch.tensor(self.labels[idx]), len(self.sequences[idx])\n",
    "\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(1, batch_size, self.hidden_dim).requires_grad_()\n",
    "        c0 = torch.zeros(1, batch_size, self.hidden_dim).requires_grad_()\n",
    "\n",
    "        x = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True)\n",
    "\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        out = self.fc(hn[-1])\n",
    "        return out\n",
    "\n",
    "    \n",
    "vectorizer = CountVectorizer(stop_words='english', lowercase=True)\n",
    "X_train = vectorizer.fit_transform(sentences_train)\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dim = 100\n",
    "output_dim = 2\n",
    "lr = 0.01\n",
    "num_epochs = 10\n",
    "\n",
    "\n",
    "model = LSTMClassifier(input_dim, hidden_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "train_dataset = TextDataset(sentences_train, y_train, vectorizer, input_dim)\n",
    "test_dataset = TextDataset(sentences_test, y_test, vectorizer, input_dim)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels, lengths) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs.float(), lengths)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    pred = []\n",
    "    for i, (inputs, labels, lengths) in enumerate(test_loader):\n",
    "        outputs = model(inputs.float(), lengths)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        pred.append(predicted)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "y_pred = np.asarray(torch.cat(pred))\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=['NOT', 'OFF']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d8b59bf2e713f17ff47b76154068196e0642fc6c62d3839f6eb91aeb6e90fd05"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
