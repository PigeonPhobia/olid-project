{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import preprocess, plotutils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data\\olid-training-v1.0.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_label(a, b, c):\n",
    "    if a == 'NOT':\n",
    "        return a\n",
    "    elif b == 'UNT':\n",
    "        return b\n",
    "    else:\n",
    "        return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df.apply(lambda x: compute_label(x.subtask_a, x.subtask_b, x.subtask_c), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = df.groupby('label').size().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pie(x=count.values(), autopct='%.0f%%')\n",
    "plt.legend(count.keys())\n",
    "plt.savefig(\"1.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_a_labels = df['subtask_a'].value_counts().to_dict()\n",
    "sub_b_labels = df['subtask_b'].value_counts().to_dict()\n",
    "sub_c_labels = df['subtask_c'].value_counts().to_dict()\n",
    "\n",
    "plotutils.plot_data_sample_bar_chart(sub_a_labels, sub_b_labels, sub_c_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_tweets = [len(tweet) for tweet in df[\"tweet\"].tolist()]\n",
    "plt.hist(len_tweets, bins=30)\n",
    "plt.title('Number of characters')\n",
    "plt.xlabel('Number of characters')\n",
    "plt.ylabel('Number of tweets')\n",
    "plt.savefig(\"num_char.svg\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "#tweet_tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, match_phone_numbers=False)\n",
    "#docs = [tweet_tokenizer.tokenize(tweet) for tweet in df.tweet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import stop_words\n",
    "from string import punctuation\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stop_words.STOP_WORDS\n",
    "stop_words = stop_words.union(set(emoji.get_emoji_unicode_dict('en').values()))\n",
    "stop_words.add(\"URL\")\n",
    "stop_words.add(\"url\")\n",
    "stop_words = stop_words.union(set(punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_tokenization(doc):\n",
    "    tweet_tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, match_phone_numbers=False)\n",
    "    tweet_tokenize = [tweet_tokenizer.tokenize(tweet) for tweet in doc]\n",
    "    return tweet_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_word_removal(doc, stop_words):\n",
    "    new_tweets = []\n",
    "    for t in doc:\n",
    "        new_words = []\n",
    "        for word in t:\n",
    "            if word not in stop_words:\n",
    "                new_words.append(word)\n",
    "        new_tweets.append(new_words)\n",
    "\n",
    "    return new_tweets\n",
    "\n",
    "    #docs = [[word for word in doc if word not in stop_words] for doc in tweet_tokenize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashtag_removal(doc):\n",
    "    new_docs = []\n",
    "    for t in doc:\n",
    "        new_doc=[]\n",
    "        for word in t:\n",
    "            if word[0] == \"#\":\n",
    "                new_doc.append(word[1:])\n",
    "            else:\n",
    "                new_doc.append(word)\n",
    "        new_docs.append(new_doc)\n",
    "    \n",
    "    return new_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokenization = tweet_tokenization(df[\"tweet\"])\n",
    "tweet_remove_stop_word = stop_word_removal(tweet_tokenization, stop_words)\n",
    "tweet_remove = hashtag_removal(tweet_remove_stop_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n",
    "    return \"hsl(210, 100%, 24%)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_wordcloud(tweet_remove):\n",
    "    text = \" \".join([word for doc in tweet_remove for word in doc])\n",
    "    wordcloud = WordCloud(background_color='white', color_func=color_func, width=1600, height=800).generate(text)\n",
    "\n",
    "    # Display the generated image:\n",
    "    plt.figure( figsize=(20, 10) )\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(\"cloud_TIN.svg\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_tweets = [len(tweet) for tweet in tweet_remove]\n",
    "plt.hist(len_tweets, bins=30)\n",
    "plt.title('Number of tokens')\n",
    "plt.xlabel('Number of tokens')\n",
    "plt.ylabel('Number of tweets')\n",
    "plt.savefig(\"num_token.svg\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_wordcloud(tweet_remove)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not Offensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not_offensive = df[df[\"subtask_a\"] == \"NOT\"][\"tweet\"]\n",
    "# print(not_offensive)\n",
    "a = df[df[\"subtask_a\"] == \"NOT\"]\n",
    "b = a[\"tweet\"]\n",
    "tweet_tokenization = tweet_tokenization(b)\n",
    "tweet_remove_stop_word = stop_word_removal(tweet_tokenization, stop_words)\n",
    "tweet_remove = hashtag_removal(tweet_remove_stop_word)\n",
    "draw_wordcloud(tweet_remove)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Offensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offensive = df[df[\"subtask_a\"] == \"OFF\"][\"tweet\"]\n",
    "tweet_tokenization = tweet_tokenization(offensive)\n",
    "tweet_remove_stop_word = stop_word_removal(tweet_tokenization, stop_words)\n",
    "tweet_remove = hashtag_removal(tweet_remove_stop_word)\n",
    "draw_wordcloud(tweet_remove)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Targeted Offensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targeted_offensive = df[df[\"subtask_b\"] == \"TIN\"][\"tweet\"]\n",
    "tweet_tokenization = tweet_tokenization(targeted_offensive)\n",
    "tweet_remove_stop_word = stop_word_removal(tweet_tokenization, stop_words)\n",
    "tweet_remove = hashtag_removal(tweet_remove_stop_word)\n",
    "draw_wordcloud(tweet_remove)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual Offensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_offensive = df[df[\"subtask_c\"] == \"IND\"][\"tweet\"]\n",
    "tweet_tokenization = tweet_tokenization(individual_offensive)\n",
    "tweet_remove_stop_word = stop_word_removal(tweet_tokenization, stop_words)\n",
    "tweet_remove = hashtag_removal(tweet_remove_stop_word)\n",
    "draw_wordcloud(tweet_remove)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group Offensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_offensive = df[df[\"subtask_c\"] == \"GRP\"][\"tweet\"]\n",
    "tweet_tokenization = tweet_tokenization(group_offensive)\n",
    "tweet_remove_stop_word = stop_word_removal(tweet_tokenization, stop_words)\n",
    "tweet_remove = hashtag_removal(tweet_remove_stop_word)\n",
    "draw_wordcloud(tweet_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_offensive = df[df[\"subtask_c\"] == \"OTH\"][\"tweet\"]\n",
    "tweet_tokenization = tweet_tokenization(group_offensive)\n",
    "tweet_remove_stop_word = stop_word_removal(tweet_tokenization, stop_words)\n",
    "tweet_remove = hashtag_removal(tweet_remove_stop_word)\n",
    "draw_wordcloud(tweet_remove)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9176542ba6fa2d56b5afe656be9ce5ab396bcf4749516db719ee5325dca786ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
